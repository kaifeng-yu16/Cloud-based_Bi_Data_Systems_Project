{"cells":[{"cell_type":"markdown","source":["# XGBoost training\nThis is an auto-generated notebook. To reproduce these results, attach this notebook to the **ML-singlenode** cluster and rerun it.\n- Compare trials in the [MLflow experiment](#mlflow/experiments/65649428652650/s?orderByKey=metrics.%60val_f1_score%60&orderByAsc=false)\n- Navigate to the parent notebook [here](#notebook/65649428652649) (If you launched the AutoML experiment using the Experiments UI, this link isn't very useful.)\n- Clone this notebook into your project folder by selecting **File > Clone** in the notebook toolbar.\n\nRuntime Version: _10.3.x-cpu-ml-scala2.12_"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b60bfaac-bdf5-40f5-b2c3-a773fbfa430d"}}},{"cell_type":"code","source":["import mlflow\nimport databricks.automl_runtime\n\n# Use MLflow to track experiments\nmlflow.set_experiment(\"/Users/ky99@duke.edu/databricks_automl/type_caffeine_processed_csv-2022_03_27-20_46\")\n\ntarget_col = \"type\""],"metadata":{"execution":{"iopub.execute_input":"2022-03-28T00:48:23.927396Z","iopub.status.busy":"2022-03-28T00:48:23.926048Z","iopub.status.idle":"2022-03-28T00:48:24.295412Z","shell.execute_reply":"2022-03-28T00:48:24.296509Z"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2c910c4-a52e-4625-8e50-30c05cd2a2b7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Load Data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21f19b9a-5c9c-4fa5-8ada-ac58416d4802"}}},{"cell_type":"code","source":["from mlflow.tracking import MlflowClient\nimport os\nimport uuid\nimport shutil\nimport pandas as pd\n\n# Create temp directory to download input data from MLflow\ninput_temp_dir = os.path.join(os.environ[\"SPARK_LOCAL_DIRS\"], \"tmp\", str(uuid.uuid4())[:8])\nos.makedirs(input_temp_dir)\n\n\n# Download the artifact and read it into a pandas DataFrame\ninput_client = MlflowClient()\ninput_data_path = input_client.download_artifacts(\"fc633788a962456db33fc2e866adf6be\", \"data\", input_temp_dir)\n\ndf_loaded = pd.read_parquet(os.path.join(input_data_path, \"training_data\"))\n# Delete the temp data\nshutil.rmtree(input_temp_dir)\n\n# Preview data\ndf_loaded.head(5)"],"metadata":{"execution":{"iopub.execute_input":"2022-03-28T00:48:24.301553Z","iopub.status.busy":"2022-03-28T00:48:24.299976Z","iopub.status.idle":"2022-03-28T00:48:24.911659Z","shell.execute_reply":"2022-03-28T00:48:24.910209Z"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2104b1d9-75aa-4b03-b535-63107a2b309c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>volume</th>\n      <th>calories</th>\n      <th>caffeine</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>256.993715</td>\n      <td>0</td>\n      <td>277</td>\n      <td>Coffee</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>250.191810</td>\n      <td>0</td>\n      <td>145</td>\n      <td>Coffee</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>250.191810</td>\n      <td>150</td>\n      <td>100</td>\n      <td>Coffee</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>250.191810</td>\n      <td>0</td>\n      <td>430</td>\n      <td>Coffee</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>250.191810</td>\n      <td>0</td>\n      <td>66</td>\n      <td>Coffee</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>volume</th>\n      <th>calories</th>\n      <th>caffeine</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>256.993715</td>\n      <td>0</td>\n      <td>277</td>\n      <td>Coffee</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>250.191810</td>\n      <td>0</td>\n      <td>145</td>\n      <td>Coffee</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>250.191810</td>\n      <td>150</td>\n      <td>100</td>\n      <td>Coffee</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>250.191810</td>\n      <td>0</td>\n      <td>430</td>\n      <td>Coffee</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>250.191810</td>\n      <td>0</td>\n      <td>66</td>\n      <td>Coffee</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Select supported columns\nSelect only the columns that are supported. This allows us to train a model that can predict on a dataset that has extra columns that are not used in training.\n`[]` are dropped in the pipelines. See the Alerts tab of the AutoML Experiment page for details on why these columns are dropped."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65b29ad0-0328-40b4-9978-414482f70674"}}},{"cell_type":"code","source":["from databricks.automl_runtime.sklearn.column_selector import ColumnSelector\nsupported_cols = [\"caffeine\", \"volume\", \"calories\"]\ncol_selector = ColumnSelector(supported_cols)"],"metadata":{"execution":{"iopub.execute_input":"2022-03-28T00:48:24.917663Z","iopub.status.busy":"2022-03-28T00:48:24.916790Z","iopub.status.idle":"2022-03-28T00:48:25.345901Z","shell.execute_reply":"2022-03-28T00:48:25.346381Z"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8bcadabd-e636-45ca-9db1-fb765f6ae5d1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Preprocessors"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7a0a64c-228d-43a5-bf46-d0aafda8ba03"}}},{"cell_type":"code","source":["transformers = []"],"metadata":{"execution":{"iopub.execute_input":"2022-03-28T00:48:25.351807Z","iopub.status.busy":"2022-03-28T00:48:25.351063Z","iopub.status.idle":"2022-03-28T00:48:25.359128Z","shell.execute_reply":"2022-03-28T00:48:25.360240Z"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"786dacd7-dec0-4800-8399-489d1e134b86"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Numerical columns\n\nMissing values for numerical columns are imputed with mean for consistency"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c15187d-babd-41a0-b7e6-d3bc36f07d29"}}},{"cell_type":"code","source":["from sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\n\nnumerical_pipeline = Pipeline(steps=[\n    (\"converter\", FunctionTransformer(lambda df: df.apply(pd.to_numeric, errors=\"coerce\"))),\n    (\"imputer\", SimpleImputer(strategy=\"mean\"))\n])\n\ntransformers.append((\"numerical\", numerical_pipeline, [\"caffeine\", \"volume\", \"calories\"]))"],"metadata":{"execution":{"iopub.execute_input":"2022-03-28T00:48:25.364820Z","iopub.status.busy":"2022-03-28T00:48:25.363375Z","iopub.status.idle":"2022-03-28T00:48:25.456555Z","shell.execute_reply":"2022-03-28T00:48:25.457569Z"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4db1de34-6257-41d1-a1af-41679d946835"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers, remainder=\"passthrough\", sparse_threshold=0)"],"metadata":{"execution":{"iopub.execute_input":"2022-03-28T00:48:25.461846Z","iopub.status.busy":"2022-03-28T00:48:25.460626Z","iopub.status.idle":"2022-03-28T00:48:25.468537Z","shell.execute_reply":"2022-03-28T00:48:25.469427Z"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80c75c53-08c9-4f8b-acdd-36f018dd3f11"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Feature standardization\nScale all feature columns to be centered around zero with unit variance."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71e39583-fbd9-4e0d-ab6a-e437254e02b2"}}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n\nstandardizer = StandardScaler()"],"metadata":{"execution":{"iopub.execute_input":"2022-03-28T00:48:25.473463Z","iopub.status.busy":"2022-03-28T00:48:25.472221Z","iopub.status.idle":"2022-03-28T00:48:25.477342Z","shell.execute_reply":"2022-03-28T00:48:25.478180Z"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0f9b9af-7523-4406-929c-04236980f3b8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Train - Validation - Test Split\nSplit the input data into 3 sets:\n- Train (60% of the dataset used to train the model)\n- Validation (20% of the dataset used to tune the hyperparameters of the model)\n- Test (20% of the dataset used to report the true performance of the model on an unseen dataset)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba4a186f-7741-4009-9fc4-4612d487f183"}}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n\nsplit_X = df_loaded.drop([target_col], axis=1)\nsplit_y = df_loaded[target_col]\n\n# Split out train data\nX_train, split_X_rem, y_train, split_y_rem = train_test_split(split_X, split_y, train_size=0.6, random_state=306722101, stratify=split_y)\n\n# Split remaining data equally for validation and test\nX_val, X_test, y_val, y_test = train_test_split(split_X_rem, split_y_rem, test_size=0.5, random_state=306722101, stratify=split_y_rem)"],"metadata":{"execution":{"iopub.execute_input":"2022-03-28T00:48:25.483793Z","iopub.status.busy":"2022-03-28T00:48:25.481373Z","iopub.status.idle":"2022-03-28T00:48:25.497058Z","shell.execute_reply":"2022-03-28T00:48:25.497914Z"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6524975-dd1d-4249-ba98-59b23b142bf0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Train classification model\n- Log relevant metrics to MLflow to track runs\n- All the runs are logged under [this MLflow experiment](#mlflow/experiments/65649428652650/s?orderByKey=metrics.%60val_f1_score%60&orderByAsc=false)\n- Change the model parameters and re-run the training cell to log a different trial to the MLflow experiment\n- To view the full list of tunable hyperparameters, check the output of the cell below"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f62d29cd-4eaa-41ea-9462-69a51cb19edd"}}},{"cell_type":"code","source":["from xgboost import XGBClassifier\n\nhelp(XGBClassifier)"],"metadata":{"execution":{"iopub.execute_input":"2022-03-28T00:48:25.501947Z","iopub.status.busy":"2022-03-28T00:48:25.500619Z","iopub.status.idle":"2022-03-28T00:48:25.615415Z","shell.execute_reply":"2022-03-28T00:48:25.616109Z"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2db4e947-d4cb-4bc6-a3bd-078f5012ca6b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Help on class XGBClassifier in module xgboost.sklearn:\n\nclass XGBClassifier(XGBModel, sklearn.base.ClassifierMixin)\n |  XGBClassifier(*, objective: Union[str, Callable[[numpy.ndarray, numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'binary:logistic', use_label_encoder: bool = True, **kwargs: Any) -> None\n |  \n |  Implementation of the scikit-learn API for XGBoost classification.\n |  \n |  \n |  Parameters\n |  ----------\n |  \n |      n_estimators : int\n |          Number of boosting rounds.\n |      use_label_encoder : bool\n |          (Deprecated) Use the label encoder from scikit-learn to encode the labels. For new\n |          code, we recommend that you set this parameter to False.\n |  \n |      max_depth :  Optional[int]\n |          Maximum tree depth for base learners.\n |      learning_rate : Optional[float]\n |          Boosting learning rate (xgb's \"eta\")\n |      verbosity : Optional[int]\n |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n |      objective : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n |          Specify the learning task and the corresponding learning objective or\n |          a custom objective function to be used (see note below).\n |      booster: Optional[str]\n |          Specify which booster to use: gbtree, gblinear or dart.\n |      tree_method: Optional[str]\n |          Specify which tree method to use.  Default to auto.  If this parameter\n |          is set to default, XGBoost will choose the most conservative option\n |          available.  It's recommended to study this option from the parameters\n |          document: https://xgboost.readthedocs.io/en/latest/treemethod.html.\n |      n_jobs : Optional[int]\n |          Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n |          algorithms like grid search, you may choose which algorithm to parallelize and\n |          balance the threads.  Creating thread contention will significantly slow down both\n |          algorithms.\n |      gamma : Optional[float]\n |          Minimum loss reduction required to make a further partition on a leaf\n |          node of the tree.\n |      min_child_weight : Optional[float]\n |          Minimum sum of instance weight(hessian) needed in a child.\n |      max_delta_step : Optional[float]\n |          Maximum delta step we allow each tree's weight estimation to be.\n |      subsample : Optional[float]\n |          Subsample ratio of the training instance.\n |      colsample_bytree : Optional[float]\n |          Subsample ratio of columns when constructing each tree.\n |      colsample_bylevel : Optional[float]\n |          Subsample ratio of columns for each level.\n |      colsample_bynode : Optional[float]\n |          Subsample ratio of columns for each split.\n |      reg_alpha : Optional[float]\n |          L1 regularization term on weights (xgb's alpha).\n |      reg_lambda : Optional[float]\n |          L2 regularization term on weights (xgb's lambda).\n |      scale_pos_weight : Optional[float]\n |          Balancing of positive and negative weights.\n |      base_score : Optional[float]\n |          The initial prediction score of all instances, global bias.\n |      random_state : Optional[Union[numpy.random.RandomState, int]]\n |          Random number seed.\n |  \n |          .. note::\n |  \n |             Using gblinear booster with shotgun updater is nondeterministic as\n |             it uses Hogwild algorithm.\n |  \n |      missing : float, default np.nan\n |          Value in the data which needs to be present as a missing value.\n |      num_parallel_tree: Optional[int]\n |          Used for boosting random forest.\n |      monotone_constraints : Optional[Union[Dict[str, int], str]]\n |          Constraint of variable monotonicity.  See tutorial for more\n |          information.\n |      interaction_constraints : Optional[Union[str, List[Tuple[str]]]]\n |          Constraints for interaction representing permitted interactions.  The\n |          constraints must be specified in the form of a nest list, e.g. [[0, 1],\n |          [2, 3, 4]], where each inner list is a group of indices of features\n |          that are allowed to interact with each other.  See tutorial for more\n |          information\n |      importance_type: Optional[str]\n |          The feature importance type for the feature_importances\\_ property:\n |  \n |          * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n |            \"total_cover\".\n |          * For linear model, only \"weight\" is defined and it's the normalized coefficients\n |            without bias.\n |  \n |      gpu_id : Optional[int]\n |          Device ordinal.\n |      validate_parameters : Optional[bool]\n |          Give warnings for unknown parameter.\n |      predictor : Optional[str]\n |          Force XGBoost to use specific predictor, available choices are [cpu_predictor,\n |          gpu_predictor].\n |      enable_categorical : bool\n |  \n |          .. versionadded:: 1.5.0\n |  \n |          Experimental support for categorical data.  Do not set to true unless you are\n |          interested in development. Only valid when `gpu_hist` and dataframe are used.\n |  \n |      kwargs : dict, optional\n |          Keyword arguments for XGBoost Booster object.  Full documentation of\n |          parameters can be found here:\n |          https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n |          dict simultaneously will result in a TypeError.\n |  \n |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n |  \n |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n |              that parameters passed via this argument will interact properly\n |              with scikit-learn.\n |  \n |          .. note::  Custom objective function\n |  \n |              A custom objective function can be provided for the ``objective``\n |              parameter. In this case, it should have the signature\n |              ``objective(y_true, y_pred) -> grad, hess``:\n |  \n |              y_true: array_like of shape [n_samples]\n |                  The target values\n |              y_pred: array_like of shape [n_samples]\n |                  The predicted values\n |  \n |              grad: array_like of shape [n_samples]\n |                  The value of the gradient for each sample point.\n |              hess: array_like of shape [n_samples]\n |                  The value of the second derivative for each sample point\n |  \n |  Method resolution order:\n |      XGBClassifier\n |      XGBModel\n |      sklearn.base.BaseEstimator\n |      sklearn.base.ClassifierMixin\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, *, objective: Union[str, Callable[[numpy.ndarray, numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'binary:logistic', use_label_encoder: bool = True, **kwargs: Any) -> None\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  evals_result(self) -> Dict[str, Dict[str, Union[List[float], List[Tuple[float, float]]]]]\n |      Return the evaluation results.\n |      \n |      If **eval_set** is passed to the `fit` function, you can call\n |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n |      When **eval_metric** is also passed to the `fit` function, the\n |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n |      \n |      Returns\n |      -------\n |      evals_result : dictionary\n |      \n |      Example\n |      -------\n |      \n |      .. code-block:: python\n |      \n |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n |      \n |          clf = xgb.XGBClassifier(**param_dist)\n |      \n |          clf.fit(X_train, y_train,\n |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n |                  eval_metric='logloss',\n |                  verbose=True)\n |      \n |          evals_result = clf.evals_result()\n |      \n |      The variable **evals_result** will contain\n |      \n |      .. code-block:: python\n |      \n |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n |  \n |  fit(self, X: Any, y: Any, *, sample_weight: Union[Any, NoneType] = None, base_margin: Union[Any, NoneType] = None, eval_set: Union[List[Tuple[Any, Any]], NoneType] = None, eval_metric: Union[str, List[str], Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[str, float]], NoneType] = None, early_stopping_rounds: Union[int, NoneType] = None, verbose: Union[bool, NoneType] = True, xgb_model: Union[xgboost.core.Booster, str, xgboost.sklearn.XGBModel, NoneType] = None, sample_weight_eval_set: Union[List[Any], NoneType] = None, base_margin_eval_set: Union[List[Any], NoneType] = None, feature_weights: Union[Any, NoneType] = None, callbacks: Union[List[xgboost.callback.TrainingCallback], NoneType] = None) -> 'XGBClassifier'\n |      Fit gradient boosting classifier.\n |      \n |      Note that calling ``fit()`` multiple times will cause the model object to be\n |      re-fit from scratch. To resume training from a previous checkpoint, explicitly\n |      pass ``xgb_model`` argument.\n |      \n |      Parameters\n |      ----------\n |      X :\n |          Feature matrix\n |      y :\n |          Labels\n |      sample_weight :\n |          instance weights\n |      base_margin :\n |          global bias for each instance.\n |      eval_set :\n |          A list of (X, y) tuple pairs to use as validation sets, for which\n |          metrics will be computed.\n |          Validation metrics will help us track the performance of the model.\n |      eval_metric :\n |          If a str, should be a built-in evaluation metric to use. See doc/parameter.rst.\n |      \n |          If a list of str, should be the list of multiple built-in evaluation metrics\n |          to use.\n |      \n |          If callable, a custom evaluation metric. The call signature is\n |          ``func(y_predicted, y_true)`` where ``y_true`` will be a DMatrix object such\n |          that you may need to call the ``get_label`` method. It must return a str,\n |          value pair where the str is a name for the evaluation and value is the value\n |          of the evaluation function. The callable custom objective is always minimized.\n |      early_stopping_rounds :\n |          Activates early stopping. Validation metric needs to improve at least once in\n |          every **early_stopping_rounds** round(s) to continue training.\n |          Requires at least one item in **eval_set**.\n |      \n |          The method returns the model from the last iteration (not the best one).\n |          If there's more than one item in **eval_set**, the last entry will be used\n |          for early stopping.\n |      \n |          If there's more than one metric in **eval_metric**, the last metric will be\n |          used for early stopping.\n |      \n |          If early stopping occurs, the model will have three additional fields:\n |          ``clf.best_score``, ``clf.best_iteration``.\n |      verbose :\n |          If `verbose` and an evaluation set is used, writes the evaluation metric\n |          measured on the validation set to stderr.\n |      xgb_model :\n |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n |          loaded before training (allows training continuation).\n |      sample_weight_eval_set :\n |          A list of the form [L_1, L_2, ..., L_n], where each L_i is an array like\n |          object storing instance weights for the i-th validation set.\n |      base_margin_eval_set :\n |          A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n |          object storing base margin for the i-th validation set.\n |      feature_weights :\n |          Weight for each feature, defines the probability of each feature being\n |          selected when colsample is being used.  All values must be greater than 0,\n |          otherwise a `ValueError` is thrown.  Only available for `hist`, `gpu_hist` and\n |          `exact` tree methods.\n |      callbacks :\n |          List of callback functions that are applied at end of each iteration.\n |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n |          Example:\n |      \n |          .. code-block:: python\n |      \n |              callbacks = [xgb.callback.EarlyStopping(rounds=early_stopping_rounds,\n |                                                      save_best=True)]\n |  \n |  predict(self, X: Any, output_margin: bool = False, ntree_limit: Union[int, NoneType] = None, validate_features: bool = True, base_margin: Union[Any, NoneType] = None, iteration_range: Union[Tuple[int, int], NoneType] = None) -> numpy.ndarray\n |      Predict with `X`.  If the model is trained with early stopping, then `best_iteration`\n |      is used automatically.  For tree models, when data is on GPU, like cupy array or\n |      cuDF dataframe and `predictor` is not specified, the prediction is run on GPU\n |      automatically, otherwise it will run on CPU.\n |      \n |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n |      \n |      Parameters\n |      ----------\n |      X :\n |          Data to predict with.\n |      output_margin :\n |          Whether to output the raw untransformed margin value.\n |      ntree_limit :\n |          Deprecated, use `iteration_range` instead.\n |      validate_features :\n |          When this is True, validate that the Booster's and data's feature_names are\n |          identical.  Otherwise, it is assumed that the feature_names are the same.\n |      base_margin :\n |          Margin added to prediction.\n |      iteration_range :\n |          Specifies which layer of trees are used in prediction.  For example, if a\n |          random forest is trained with 100 rounds.  Specifying ``iteration_range=(10,\n |          20)``, then only the forests built during [10, 20) (half open set) rounds are\n |          used in this prediction.\n |      \n |          .. versionadded:: 1.4.0\n |      \n |      Returns\n |      -------\n |      prediction\n |  \n |  predict_proba(self, X: Any, ntree_limit: Union[int, NoneType] = None, validate_features: bool = True, base_margin: Union[Any, NoneType] = None, iteration_range: Union[Tuple[int, int], NoneType] = None) -> numpy.ndarray\n |      Predict the probability of each `X` example being of a given class.\n |      \n |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n |      \n |      Parameters\n |      ----------\n |      X : array_like\n |          Feature matrix.\n |      ntree_limit : int\n |          Deprecated, use `iteration_range` instead.\n |      validate_features : bool\n |          When this is True, validate that the Booster's and data's feature_names are\n |          identical.  Otherwise, it is assumed that the feature_names are the same.\n |      base_margin : array_like\n |          Margin added to prediction.\n |      iteration_range :\n |          Specifies which layer of trees are used in prediction.  For example, if a\n |          random forest is trained with 100 rounds.  Specifying `iteration_range=(10,\n |          20)`, then only the forests built during [10, 20) (half open set) rounds are\n |          used in this prediction.\n |      \n |      Returns\n |      -------\n |      prediction :\n |          a numpy array of shape array-like of shape (n_samples, n_classes) with the\n |          probability of each data example being of a given class.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from XGBModel:\n |  \n |  __sklearn_is_fitted__(self) -> bool\n |  \n |  apply(self, X: Any, ntree_limit: int = 0, iteration_range: Union[Tuple[int, int], NoneType] = None) -> numpy.ndarray\n |      Return the predicted leaf every tree for each sample. If the model is trained with\n |      early stopping, then `best_iteration` is used automatically.\n |      \n |      Parameters\n |      ----------\n |      X : array_like, shape=[n_samples, n_features]\n |          Input features matrix.\n |      \n |      iteration_range :\n |          See :py:meth:`xgboost.XGBRegressor.predict`.\n |      \n |      ntree_limit :\n |          Deprecated, use ``iteration_range`` instead.\n |      \n |      Returns\n |      -------\n |      X_leaves : array_like, shape=[n_samples, n_trees]\n |          For each datapoint x in X and for each tree, return the index of the\n |          leaf x ends up in. Leaves are numbered within\n |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n |  \n |  get_booster(self) -> xgboost.core.Booster\n |      Get the underlying xgboost Booster of this model.\n |      \n |      This will raise an exception when fit was not called\n |      \n |      Returns\n |      -------\n |      booster : a xgboost booster of underlying model\n |  \n |  get_num_boosting_rounds(self) -> int\n |      Gets the number of xgboost boosting rounds.\n |  \n |  get_params(self, deep: bool = True) -> Dict[str, Any]\n |      Get parameters.\n |  \n |  get_xgb_params(self) -> Dict[str, Any]\n |      Get xgboost specific parameters.\n |  \n |  load_model(self, fname: Union[str, bytearray, os.PathLike]) -> None\n |      Load the model from a file or bytearray. Path to file can be local\n |      or as an URI.\n |      \n |      The model is loaded from XGBoost format which is universal among the various\n |      XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as\n |      feature_names) will not be loaded when using binary format.  To save those\n |      attributes, use JSON instead.  See: `Model IO\n |      <https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html>`_ for more\n |      info.\n |      \n |      Parameters\n |      ----------\n |      fname :\n |          Input file name or memory buffer(see also save_raw)\n |  \n |  save_model(self, fname: Union[str, os.PathLike]) -> None\n |      Save the model to a file.\n |      \n |      The model is saved in an XGBoost internal format which is universal among the\n |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n |      (such as feature_names) will not be saved when using binary format.  To save those\n |      attributes, use JSON instead. See: `Model IO\n |      <https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html>`_ for more\n |      info.\n |      \n |      Parameters\n |      ----------\n |      fname : string or os.PathLike\n |          Output file name\n |  \n |  set_params(self, **params: Any) -> 'XGBModel'\n |      Set the parameters of this estimator.  Modification of the sklearn method to\n |      allow unknown kwargs. This allows using the full range of xgboost\n |      parameters that are not defined as member variables in sklearn grid\n |      search.\n |      \n |      Returns\n |      -------\n |      self\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from XGBModel:\n |  \n |  best_iteration\n |  \n |  best_ntree_limit\n |  \n |  best_score\n |  \n |  coef_\n |      Coefficients property\n |      \n |      .. note:: Coefficients are defined only for linear learners\n |      \n |          Coefficients are only defined when the linear model is chosen as\n |          base learner (`booster=gblinear`). It is not defined for other base\n |          learner types, such as tree learners (`booster=gbtree`).\n |      \n |      Returns\n |      -------\n |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n |  \n |  feature_importances_\n |      Feature importances property, return depends on `importance_type` parameter.\n |      \n |      Returns\n |      -------\n |      feature_importances_ : array of shape ``[n_features]`` except for multi-class\n |      linear model, which returns an array with shape `(n_features, n_classes)`\n |  \n |  intercept_\n |      Intercept (bias) property\n |      \n |      .. note:: Intercept is defined only for linear learners\n |      \n |          Intercept (bias) is only defined when the linear model is chosen as base\n |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n |          as tree learners (`booster=gbtree`).\n |      \n |      Returns\n |      -------\n |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n |  \n |  n_features_in_\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.BaseEstimator:\n |  \n |  __getstate__(self)\n |  \n |  __repr__(self, N_CHAR_MAX=700)\n |      Return repr(self).\n |  \n |  __setstate__(self, state)\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from sklearn.base.BaseEstimator:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.ClassifierMixin:\n |  \n |  score(self, X, y, sample_weight=None)\n |      Return the mean accuracy on the given test data and labels.\n |      \n |      In multi-label classification, this is the subset accuracy\n |      which is a harsh metric since you require for each sample that\n |      each label set be correctly predicted.\n |      \n |      Parameters\n |      ----------\n |      X : array-like of shape (n_samples, n_features)\n |          Test samples.\n |      \n |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n |          True labels for `X`.\n |      \n |      sample_weight : array-like of shape (n_samples,), default=None\n |          Sample weights.\n |      \n |      Returns\n |      -------\n |      score : float\n |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Help on class XGBClassifier in module xgboost.sklearn:\n\nclass XGBClassifier(XGBModel, sklearn.base.ClassifierMixin)\n |  XGBClassifier(*, objective: Union[str, Callable[[numpy.ndarray, numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'binary:logistic', use_label_encoder: bool = True, **kwargs: Any) -> None\n |  \n |  Implementation of the scikit-learn API for XGBoost classification.\n |  \n |  \n |  Parameters\n |  ----------\n |  \n |      n_estimators : int\n |          Number of boosting rounds.\n |      use_label_encoder : bool\n |          (Deprecated) Use the label encoder from scikit-learn to encode the labels. For new\n |          code, we recommend that you set this parameter to False.\n |  \n |      max_depth :  Optional[int]\n |          Maximum tree depth for base learners.\n |      learning_rate : Optional[float]\n |          Boosting learning rate (xgb's \"eta\")\n |      verbosity : Optional[int]\n |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n |      objective : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n |          Specify the learning task and the corresponding learning objective or\n |          a custom objective function to be used (see note below).\n |      booster: Optional[str]\n |          Specify which booster to use: gbtree, gblinear or dart.\n |      tree_method: Optional[str]\n |          Specify which tree method to use.  Default to auto.  If this parameter\n |          is set to default, XGBoost will choose the most conservative option\n |          available.  It's recommended to study this option from the parameters\n |          document: https://xgboost.readthedocs.io/en/latest/treemethod.html.\n |      n_jobs : Optional[int]\n |          Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n |          algorithms like grid search, you may choose which algorithm to parallelize and\n |          balance the threads.  Creating thread contention will significantly slow down both\n |          algorithms.\n |      gamma : Optional[float]\n |          Minimum loss reduction required to make a further partition on a leaf\n |          node of the tree.\n |      min_child_weight : Optional[float]\n |          Minimum sum of instance weight(hessian) needed in a child.\n |      max_delta_step : Optional[float]\n |          Maximum delta step we allow each tree's weight estimation to be.\n |      subsample : Optional[float]\n |          Subsample ratio of the training instance.\n |      colsample_bytree : Optional[float]\n |          Subsample ratio of columns when constructing each tree.\n |      colsample_bylevel : Optional[float]\n |          Subsample ratio of columns for each level.\n |      colsample_bynode : Optional[float]\n |          Subsample ratio of columns for each split.\n |      reg_alpha : Optional[float]\n |          L1 regularization term on weights (xgb's alpha).\n |      reg_lambda : Optional[float]\n |          L2 regularization term on weights (xgb's lambda).\n |      scale_pos_weight : Optional[float]\n |          Balancing of positive and negative weights.\n |      base_score : Optional[float]\n |          The initial prediction score of all instances, global bias.\n |      random_state : Optional[Union[numpy.random.RandomState, int]]\n |          Random number seed.\n |  \n |          .. note::\n |  \n |             Using gblinear booster with shotgun updater is nondeterministic as\n |             it uses Hogwild algorithm.\n |  \n |      missing : float, default np.nan\n |          Value in the data which needs to be present as a missing value.\n |      num_parallel_tree: Optional[int]\n |          Used for boosting random forest.\n |      monotone_constraints : Optional[Union[Dict[str, int], str]]\n |          Constraint of variable monotonicity.  See tutorial for more\n |          information.\n |      interaction_constraints : Optional[Union[str, List[Tuple[str]]]]\n |          Constraints for interaction representing permitted interactions.  The\n |          constraints must be specified in the form of a nest list, e.g. [[0, 1],\n |          [2, 3, 4]], where each inner list is a group of indices of features\n |          that are allowed to interact with each other.  See tutorial for more\n |          information\n |      importance_type: Optional[str]\n |          The feature importance type for the feature_importances\\_ property:\n |  \n |          * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n |            \"total_cover\".\n |          * For linear model, only \"weight\" is defined and it's the normalized coefficients\n |            without bias.\n |  \n |      gpu_id : Optional[int]\n |          Device ordinal.\n |      validate_parameters : Optional[bool]\n |          Give warnings for unknown parameter.\n |      predictor : Optional[str]\n |          Force XGBoost to use specific predictor, available choices are [cpu_predictor,\n |          gpu_predictor].\n |      enable_categorical : bool\n |  \n |          .. versionadded:: 1.5.0\n |  \n |          Experimental support for categorical data.  Do not set to true unless you are\n |          interested in development. Only valid when `gpu_hist` and dataframe are used.\n |  \n |      kwargs : dict, optional\n |          Keyword arguments for XGBoost Booster object.  Full documentation of\n |          parameters can be found here:\n |          https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n |          dict simultaneously will result in a TypeError.\n |  \n |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n |  \n |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n |              that parameters passed via this argument will interact properly\n |              with scikit-learn.\n |  \n |          .. note::  Custom objective function\n |  \n |              A custom objective function can be provided for the ``objective``\n |              parameter. In this case, it should have the signature\n |              ``objective(y_true, y_pred) -> grad, hess``:\n |  \n |              y_true: array_like of shape [n_samples]\n |                  The target values\n |              y_pred: array_like of shape [n_samples]\n |                  The predicted values\n |  \n |              grad: array_like of shape [n_samples]\n |                  The value of the gradient for each sample point.\n |              hess: array_like of shape [n_samples]\n |                  The value of the second derivative for each sample point\n |  \n |  Method resolution order:\n |      XGBClassifier\n |      XGBModel\n |      sklearn.base.BaseEstimator\n |      sklearn.base.ClassifierMixin\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, *, objective: Union[str, Callable[[numpy.ndarray, numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'binary:logistic', use_label_encoder: bool = True, **kwargs: Any) -> None\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  evals_result(self) -> Dict[str, Dict[str, Union[List[float], List[Tuple[float, float]]]]]\n |      Return the evaluation results.\n |      \n |      If **eval_set** is passed to the `fit` function, you can call\n |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n |      When **eval_metric** is also passed to the `fit` function, the\n |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n |      \n |      Returns\n |      -------\n |      evals_result : dictionary\n |      \n |      Example\n |      -------\n |      \n |      .. code-block:: python\n |      \n |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n |      \n |          clf = xgb.XGBClassifier(**param_dist)\n |      \n |          clf.fit(X_train, y_train,\n |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n |                  eval_metric='logloss',\n |                  verbose=True)\n |      \n |          evals_result = clf.evals_result()\n |      \n |      The variable **evals_result** will contain\n |      \n |      .. code-block:: python\n |      \n |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n |  \n |  fit(self, X: Any, y: Any, *, sample_weight: Union[Any, NoneType] = None, base_margin: Union[Any, NoneType] = None, eval_set: Union[List[Tuple[Any, Any]], NoneType] = None, eval_metric: Union[str, List[str], Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[str, float]], NoneType] = None, early_stopping_rounds: Union[int, NoneType] = None, verbose: Union[bool, NoneType] = True, xgb_model: Union[xgboost.core.Booster, str, xgboost.sklearn.XGBModel, NoneType] = None, sample_weight_eval_set: Union[List[Any], NoneType] = None, base_margin_eval_set: Union[List[Any], NoneType] = None, feature_weights: Union[Any, NoneType] = None, callbacks: Union[List[xgboost.callback.TrainingCallback], NoneType] = None) -> 'XGBClassifier'\n |      Fit gradient boosting classifier.\n |      \n |      Note that calling ``fit()`` multiple times will cause the model object to be\n |      re-fit from scratch. To resume training from a previous checkpoint, explicitly\n |      pass ``xgb_model`` argument.\n |      \n |      Parameters\n |      ----------\n |      X :\n |          Feature matrix\n |      y :\n |          Labels\n |      sample_weight :\n |          instance weights\n |      base_margin :\n |          global bias for each instance.\n |      eval_set :\n |          A list of (X, y) tuple pairs to use as validation sets, for which\n |          metrics will be computed.\n |          Validation metrics will help us track the performance of the model.\n |      eval_metric :\n |          If a str, should be a built-in evaluation metric to use. See doc/parameter.rst.\n |      \n |          If a list of str, should be the list of multiple built-in evaluation metrics\n |          to use.\n |      \n |          If callable, a custom evaluation metric. The call signature is\n |          ``func(y_predicted, y_true)`` where ``y_true`` will be a DMatrix object such\n |          that you may need to call the ``get_label`` method. It must return a str,\n |          value pair where the str is a name for the evaluation and value is the value\n |          of the evaluation function. The callable custom objective is always minimized.\n |      early_stopping_rounds :\n |          Activates early stopping. Validation metric needs to improve at least once in\n |          every **early_stopping_rounds** round(s) to continue training.\n |          Requires at least one item in **eval_set**.\n |      \n |          The method returns the model from the last iteration (not the best one).\n |          If there's more than one item in **eval_set**, the last entry will be used\n |          for early stopping.\n |      \n |          If there's more than one metric in **eval_metric**, the last metric will be\n |          used for early stopping.\n |      \n |          If early stopping occurs, the model will have three additional fields:\n |          ``clf.best_score``, ``clf.best_iteration``.\n |      verbose :\n |          If `verbose` and an evaluation set is used, writes the evaluation metric\n |          measured on the validation set to stderr.\n |      xgb_model :\n |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n |          loaded before training (allows training continuation).\n |      sample_weight_eval_set :\n |          A list of the form [L_1, L_2, ..., L_n], where each L_i is an array like\n |          object storing instance weights for the i-th validation set.\n |      base_margin_eval_set :\n |          A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n |          object storing base margin for the i-th validation set.\n |      feature_weights :\n |          Weight for each feature, defines the probability of each feature being\n |          selected when colsample is being used.  All values must be greater than 0,\n |          otherwise a `ValueError` is thrown.  Only available for `hist`, `gpu_hist` and\n |          `exact` tree methods.\n |      callbacks :\n |          List of callback functions that are applied at end of each iteration.\n |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n |          Example:\n |      \n |          .. code-block:: python\n |      \n |              callbacks = [xgb.callback.EarlyStopping(rounds=early_stopping_rounds,\n |                                                      save_best=True)]\n |  \n |  predict(self, X: Any, output_margin: bool = False, ntree_limit: Union[int, NoneType] = None, validate_features: bool = True, base_margin: Union[Any, NoneType] = None, iteration_range: Union[Tuple[int, int], NoneType] = None) -> numpy.ndarray\n |      Predict with `X`.  If the model is trained with early stopping, then `best_iteration`\n |      is used automatically.  For tree models, when data is on GPU, like cupy array or\n |      cuDF dataframe and `predictor` is not specified, the prediction is run on GPU\n |      automatically, otherwise it will run on CPU.\n |      \n |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n |      \n |      Parameters\n |      ----------\n |      X :\n |          Data to predict with.\n |      output_margin :\n |          Whether to output the raw untransformed margin value.\n |      ntree_limit :\n |          Deprecated, use `iteration_range` instead.\n |      validate_features :\n |          When this is True, validate that the Booster's and data's feature_names are\n |          identical.  Otherwise, it is assumed that the feature_names are the same.\n |      base_margin :\n |          Margin added to prediction.\n |      iteration_range :\n |          Specifies which layer of trees are used in prediction.  For example, if a\n |          random forest is trained with 100 rounds.  Specifying ``iteration_range=(10,\n |          20)``, then only the forests built during [10, 20) (half open set) rounds are\n |          used in this prediction.\n |      \n |          .. versionadded:: 1.4.0\n |      \n |      Returns\n |      -------\n |      prediction\n |  \n |  predict_proba(self, X: Any, ntree_limit: Union[int, NoneType] = None, validate_features: bool = True, base_margin: Union[Any, NoneType] = None, iteration_range: Union[Tuple[int, int], NoneType] = None) -> numpy.ndarray\n |      Predict the probability of each `X` example being of a given class.\n |      \n |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n |      \n |      Parameters\n |      ----------\n |      X : array_like\n |          Feature matrix.\n |      ntree_limit : int\n |          Deprecated, use `iteration_range` instead.\n |      validate_features : bool\n |          When this is True, validate that the Booster's and data's feature_names are\n |          identical.  Otherwise, it is assumed that the feature_names are the same.\n |      base_margin : array_like\n |          Margin added to prediction.\n |      iteration_range :\n |          Specifies which layer of trees are used in prediction.  For example, if a\n |          random forest is trained with 100 rounds.  Specifying `iteration_range=(10,\n |          20)`, then only the forests built during [10, 20) (half open set) rounds are\n |          used in this prediction.\n |      \n |      Returns\n |      -------\n |      prediction :\n |          a numpy array of shape array-like of shape (n_samples, n_classes) with the\n |          probability of each data example being of a given class.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from XGBModel:\n |  \n |  __sklearn_is_fitted__(self) -> bool\n |  \n |  apply(self, X: Any, ntree_limit: int = 0, iteration_range: Union[Tuple[int, int], NoneType] = None) -> numpy.ndarray\n |      Return the predicted leaf every tree for each sample. If the model is trained with\n |      early stopping, then `best_iteration` is used automatically.\n |      \n |      Parameters\n |      ----------\n |      X : array_like, shape=[n_samples, n_features]\n |          Input features matrix.\n |      \n |      iteration_range :\n |          See :py:meth:`xgboost.XGBRegressor.predict`.\n |      \n |      ntree_limit :\n |          Deprecated, use ``iteration_range`` instead.\n |      \n |      Returns\n |      -------\n |      X_leaves : array_like, shape=[n_samples, n_trees]\n |          For each datapoint x in X and for each tree, return the index of the\n |          leaf x ends up in. Leaves are numbered within\n |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n |  \n |  get_booster(self) -> xgboost.core.Booster\n |      Get the underlying xgboost Booster of this model.\n |      \n |      This will raise an exception when fit was not called\n |      \n |      Returns\n |      -------\n |      booster : a xgboost booster of underlying model\n |  \n |  get_num_boosting_rounds(self) -> int\n |      Gets the number of xgboost boosting rounds.\n |  \n |  get_params(self, deep: bool = True) -> Dict[str, Any]\n |      Get parameters.\n |  \n |  get_xgb_params(self) -> Dict[str, Any]\n |      Get xgboost specific parameters.\n |  \n |  load_model(self, fname: Union[str, bytearray, os.PathLike]) -> None\n |      Load the model from a file or bytearray. Path to file can be local\n |      or as an URI.\n |      \n |      The model is loaded from XGBoost format which is universal among the various\n |      XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as\n |      feature_names) will not be loaded when using binary format.  To save those\n |      attributes, use JSON instead.  See: `Model IO\n |      <https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html>`_ for more\n |      info.\n |      \n |      Parameters\n |      ----------\n |      fname :\n |          Input file name or memory buffer(see also save_raw)\n |  \n |  save_model(self, fname: Union[str, os.PathLike]) -> None\n |      Save the model to a file.\n |      \n |      The model is saved in an XGBoost internal format which is universal among the\n |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n |      (such as feature_names) will not be saved when using binary format.  To save those\n |      attributes, use JSON instead. See: `Model IO\n |      <https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html>`_ for more\n |      info.\n |      \n |      Parameters\n |      ----------\n |      fname : string or os.PathLike\n |          Output file name\n |  \n |  set_params(self, **params: Any) -> 'XGBModel'\n |      Set the parameters of this estimator.  Modification of the sklearn method to\n |      allow unknown kwargs. This allows using the full range of xgboost\n |      parameters that are not defined as member variables in sklearn grid\n |      search.\n |      \n |      Returns\n |      -------\n |      self\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from XGBModel:\n |  \n |  best_iteration\n |  \n |  best_ntree_limit\n |  \n |  best_score\n |  \n |  coef_\n |      Coefficients property\n |      \n |      .. note:: Coefficients are defined only for linear learners\n |      \n |          Coefficients are only defined when the linear model is chosen as\n |          base learner (`booster=gblinear`). It is not defined for other base\n |          learner types, such as tree learners (`booster=gbtree`).\n |      \n |      Returns\n |      -------\n |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n |  \n |  feature_importances_\n |      Feature importances property, return depends on `importance_type` parameter.\n |      \n |      Returns\n |      -------\n |      feature_importances_ : array of shape ``[n_features]`` except for multi-class\n |      linear model, which returns an array with shape `(n_features, n_classes)`\n |  \n |  intercept_\n |      Intercept (bias) property\n |      \n |      .. note:: Intercept is defined only for linear learners\n |      \n |          Intercept (bias) is only defined when the linear model is chosen as base\n |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n |          as tree learners (`booster=gbtree`).\n |      \n |      Returns\n |      -------\n |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n |  \n |  n_features_in_\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.BaseEstimator:\n |  \n |  __getstate__(self)\n |  \n |  __repr__(self, N_CHAR_MAX=700)\n |      Return repr(self).\n |  \n |  __setstate__(self, state)\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from sklearn.base.BaseEstimator:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.ClassifierMixin:\n |  \n |  score(self, X, y, sample_weight=None)\n |      Return the mean accuracy on the given test data and labels.\n |      \n |      In multi-label classification, this is the subset accuracy\n |      which is a harsh metric since you require for each sample that\n |      each label set be correctly predicted.\n |      \n |      Parameters\n |      ----------\n |      X : array-like of shape (n_samples, n_features)\n |          Test samples.\n |      \n |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n |          True labels for `X`.\n |      \n |      sample_weight : array-like of shape (n_samples,), default=None\n |          Sample weights.\n |      \n |      Returns\n |      -------\n |      score : float\n |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["import mlflow\nimport sklearn\nfrom sklearn import set_config\nfrom sklearn.pipeline import Pipeline\n\nset_config(display=\"diagram\")\n\nxgbc_classifier = XGBClassifier(\n  colsample_bytree=0.6670695926636986,\n  learning_rate=0.0018191097854733252,\n  max_depth=9,\n  min_child_weight=2,\n  n_estimators=933,\n  n_jobs=100,\n  subsample=0.785187475626547,\n  verbosity=0,\n  random_state=306722101,\n)\n\nmodel = Pipeline([\n    (\"column_selector\", col_selector),\n    (\"preprocessor\", preprocessor),\n    (\"standardizer\", standardizer),\n    (\"classifier\", xgbc_classifier),\n])\n\n# Create a separate pipeline to transform the validation dataset. This is used for early stopping.\npipeline = Pipeline([\n    (\"column_selector\", col_selector),\n    (\"preprocessor\", preprocessor),\n    (\"standardizer\", standardizer),\n])\n\nmlflow.sklearn.autolog(disable=True)\nX_val_processed = pipeline.fit_transform(X_val, y_val)\n\nmodel"],"metadata":{"execution":{"iopub.execute_input":"2022-03-28T00:48:25.619985Z","iopub.status.busy":"2022-03-28T00:48:25.619223Z","iopub.status.idle":"2022-03-28T00:48:25.705790Z","shell.execute_reply":"2022-03-28T00:48:25.700925Z"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3f667f7-73e4-4b6c-9a8b-884ad0ee6069"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<style>div.sk-top-container {color: black;background-color: white;}div.sk-toggleable {background-color: white;}label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}div.sk-estimator:hover {background-color: #d4ebff;}div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}div.sk-item {z-index: 1;}div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}div.sk-parallel-item:only-child::after {width: 0;}div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}div.sk-label-container {position: relative;z-index: 2;text-align: center;}div.sk-container {display: inline-block;position: relative;}</style><div class=\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"75a4ced2-a652-4a73-aa00-43615119b1e6\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"75a4ced2-a652-4a73-aa00-43615119b1e6\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('column_selector',\n                 ColumnSelector(cols=['caffeine', 'volume', 'calories'])),\n                ('preprocessor',\n                 ColumnTransformer(remainder='passthrough', sparse_threshold=0,\n                                   transformers=[('numerical',\n                                                  Pipeline(steps=[('converter',\n                                                                   FunctionTransformer(func=<function <lambda> at 0x7f713c28dc10>)),\n                                                                  ('imputer',\n                                                                   SimpleImputer())]),\n                                                  ['caffeine', 'volume',\n                                                   'calories'])])...\n                               learning_rate=0.0018191097854733252,\n                               max_delta_step=None, max_depth=9,\n                               min_child_weight=2, missing=nan,\n                               monotone_constraints=None, n_estimators=933,\n                               n_jobs=100, num_parallel_tree=None,\n                               predictor=None, random_state=306722101,\n                               reg_alpha=None, reg_lambda=None,\n                               scale_pos_weight=None,\n                               subsample=0.785187475626547, tree_method=None,\n                               validate_parameters=None, verbosity=0))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"5189ffe1-363f-486b-8869-2453af2b597e\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"5189ffe1-363f-486b-8869-2453af2b597e\">ColumnSelector</label><div class=\"sk-toggleable__content\"><pre>ColumnSelector(cols=['caffeine', 'volume', 'calories'])</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"30b8978c-c2fc-49dd-bfff-2693190adb3a\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"30b8978c-c2fc-49dd-bfff-2693190adb3a\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder='passthrough', sparse_threshold=0,\n                  transformers=[('numerical',\n                                 Pipeline(steps=[('converter',\n                                                  FunctionTransformer(func=<function <lambda> at 0x7f713c28dc10>)),\n                                                 ('imputer', SimpleImputer())]),\n                                 ['caffeine', 'volume', 'calories'])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"6539520a-9e2e-4080-933b-d0f11f74db3d\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"6539520a-9e2e-4080-933b-d0f11f74db3d\">numerical</label><div class=\"sk-toggleable__content\"><pre>['caffeine', 'volume', 'calories']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"cb16989f-1f30-4081-bee9-290e539fb975\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"cb16989f-1f30-4081-bee9-290e539fb975\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=<function <lambda> at 0x7f713c28dc10>)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"66bf6558-c37c-4c5e-8172-fd2bf7e4c431\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"66bf6558-c37c-4c5e-8172-fd2bf7e4c431\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"15d1f910-ab58-45c7-93bb-1875b0ef6f0d\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"15d1f910-ab58-45c7-93bb-1875b0ef6f0d\">remainder</label><div class=\"sk-toggleable__content\"><pre>[['caffeine', 'volume', 'calories']]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"230cf1da-c8e7-4dc9-88ef-133da2fb52c7\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"230cf1da-c8e7-4dc9-88ef-133da2fb52c7\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"47409d6c-92d3-4de9-8517-ace4d485977a\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"47409d6c-92d3-4de9-8517-ace4d485977a\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"64a697bc-5f14-4d4b-a154-3b5e48ec2e05\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"64a697bc-5f14-4d4b-a154-3b5e48ec2e05\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n              colsample_bynode=None, colsample_bytree=0.6670695926636986,\n              enable_categorical=False, gamma=None, gpu_id=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.0018191097854733252, max_delta_step=None,\n              max_depth=9, min_child_weight=2, missing=nan,\n              monotone_constraints=None, n_estimators=933, n_jobs=100,\n              num_parallel_tree=None, predictor=None, random_state=306722101,\n              reg_alpha=None, reg_lambda=None, scale_pos_weight=None,\n              subsample=0.785187475626547, tree_method=None,\n              validate_parameters=None, verbosity=0)</pre></div></div></div></div></div></div></div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style>div.sk-top-container {color: black;background-color: white;}div.sk-toggleable {background-color: white;}label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}div.sk-estimator:hover {background-color: #d4ebff;}div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}div.sk-item {z-index: 1;}div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}div.sk-parallel-item:only-child::after {width: 0;}div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}div.sk-label-container {position: relative;z-index: 2;text-align: center;}div.sk-container {display: inline-block;position: relative;}</style><div class=\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"75a4ced2-a652-4a73-aa00-43615119b1e6\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"75a4ced2-a652-4a73-aa00-43615119b1e6\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('column_selector',\n                 ColumnSelector(cols=['caffeine', 'volume', 'calories'])),\n                ('preprocessor',\n                 ColumnTransformer(remainder='passthrough', sparse_threshold=0,\n                                   transformers=[('numerical',\n                                                  Pipeline(steps=[('converter',\n                                                                   FunctionTransformer(func=<function <lambda> at 0x7f713c28dc10>)),\n                                                                  ('imputer',\n                                                                   SimpleImputer())]),\n                                                  ['caffeine', 'volume',\n                                                   'calories'])])...\n                               learning_rate=0.0018191097854733252,\n                               max_delta_step=None, max_depth=9,\n                               min_child_weight=2, missing=nan,\n                               monotone_constraints=None, n_estimators=933,\n                               n_jobs=100, num_parallel_tree=None,\n                               predictor=None, random_state=306722101,\n                               reg_alpha=None, reg_lambda=None,\n                               scale_pos_weight=None,\n                               subsample=0.785187475626547, tree_method=None,\n                               validate_parameters=None, verbosity=0))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"5189ffe1-363f-486b-8869-2453af2b597e\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"5189ffe1-363f-486b-8869-2453af2b597e\">ColumnSelector</label><div class=\"sk-toggleable__content\"><pre>ColumnSelector(cols=['caffeine', 'volume', 'calories'])</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"30b8978c-c2fc-49dd-bfff-2693190adb3a\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"30b8978c-c2fc-49dd-bfff-2693190adb3a\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder='passthrough', sparse_threshold=0,\n                  transformers=[('numerical',\n                                 Pipeline(steps=[('converter',\n                                                  FunctionTransformer(func=<function <lambda> at 0x7f713c28dc10>)),\n                                                 ('imputer', SimpleImputer())]),\n                                 ['caffeine', 'volume', 'calories'])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"6539520a-9e2e-4080-933b-d0f11f74db3d\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"6539520a-9e2e-4080-933b-d0f11f74db3d\">numerical</label><div class=\"sk-toggleable__content\"><pre>['caffeine', 'volume', 'calories']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"cb16989f-1f30-4081-bee9-290e539fb975\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"cb16989f-1f30-4081-bee9-290e539fb975\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=<function <lambda> at 0x7f713c28dc10>)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"66bf6558-c37c-4c5e-8172-fd2bf7e4c431\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"66bf6558-c37c-4c5e-8172-fd2bf7e4c431\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"15d1f910-ab58-45c7-93bb-1875b0ef6f0d\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"15d1f910-ab58-45c7-93bb-1875b0ef6f0d\">remainder</label><div class=\"sk-toggleable__content\"><pre>[['caffeine', 'volume', 'calories']]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"230cf1da-c8e7-4dc9-88ef-133da2fb52c7\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"230cf1da-c8e7-4dc9-88ef-133da2fb52c7\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"47409d6c-92d3-4de9-8517-ace4d485977a\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"47409d6c-92d3-4de9-8517-ace4d485977a\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"64a697bc-5f14-4d4b-a154-3b5e48ec2e05\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"64a697bc-5f14-4d4b-a154-3b5e48ec2e05\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n              colsample_bynode=None, colsample_bytree=0.6670695926636986,\n              enable_categorical=False, gamma=None, gpu_id=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=0.0018191097854733252, max_delta_step=None,\n              max_depth=9, min_child_weight=2, missing=nan,\n              monotone_constraints=None, n_estimators=933, n_jobs=100,\n              num_parallel_tree=None, predictor=None, random_state=306722101,\n              reg_alpha=None, reg_lambda=None, scale_pos_weight=None,\n              subsample=0.785187475626547, tree_method=None,\n              validate_parameters=None, verbosity=0)</pre></div></div></div></div></div></div></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Enable automatic logging of input samples, metrics, parameters, and models\nmlflow.sklearn.autolog(log_input_examples=True, silent=True)\n\nwith mlflow.start_run(run_name=\"xgboost\") as mlflow_run:\n    model.fit(X_train, y_train, classifier__early_stopping_rounds=5, classifier__eval_set=[(X_val_processed,y_val)], classifier__verbose=False)\n    \n    # Training metrics are logged by MLflow autologging\n    # Log metrics for the validation set\n    xgbc_val_metrics = mlflow.sklearn.eval_and_log_metrics(model, X_val, y_val, prefix=\"val_\")\n\n    # Log metrics for the test set\n    xgbc_test_metrics = mlflow.sklearn.eval_and_log_metrics(model, X_test, y_test, prefix=\"test_\")\n\n    # Display the logged metrics\n    xgbc_val_metrics = {k.replace(\"val_\", \"\"): v for k, v in xgbc_val_metrics.items()}\n    xgbc_test_metrics = {k.replace(\"test_\", \"\"): v for k, v in xgbc_test_metrics.items()}\n    display(pd.DataFrame([xgbc_val_metrics, xgbc_test_metrics], index=[\"validation\", \"test\"]))"],"metadata":{"execution":{"iopub.execute_input":"2022-03-28T00:48:25.716620Z","iopub.status.busy":"2022-03-28T00:48:25.715180Z","iopub.status.idle":"2022-03-28T00:51:32.593174Z","shell.execute_reply":"2022-03-28T00:51:32.593715Z"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"137e3cbc-24c6-49c6-bf21-09f7f15bd09d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/databricks/python/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["/databricks/python/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/databricks/python/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["/databricks/python/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>precision_score</th>\n      <th>recall_score</th>\n      <th>f1_score</th>\n      <th>accuracy_score</th>\n      <th>log_loss</th>\n      <th>roc_auc_score</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>validation</th>\n      <td>0.697709</td>\n      <td>0.729508</td>\n      <td>0.708792</td>\n      <td>0.729508</td>\n      <td>1.006351</td>\n      <td>0.914906</td>\n      <td>0.729508</td>\n    </tr>\n    <tr>\n      <th>test</th>\n      <td>0.656010</td>\n      <td>0.663934</td>\n      <td>0.653278</td>\n      <td>0.663934</td>\n      <td>1.027133</td>\n      <td>0.906787</td>\n      <td>0.663934</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>precision_score</th>\n      <th>recall_score</th>\n      <th>f1_score</th>\n      <th>accuracy_score</th>\n      <th>log_loss</th>\n      <th>roc_auc_score</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>validation</th>\n      <td>0.697709</td>\n      <td>0.729508</td>\n      <td>0.708792</td>\n      <td>0.729508</td>\n      <td>1.006351</td>\n      <td>0.914906</td>\n      <td>0.729508</td>\n    </tr>\n    <tr>\n      <th>test</th>\n      <td>0.656010</td>\n      <td>0.663934</td>\n      <td>0.653278</td>\n      <td>0.663934</td>\n      <td>1.027133</td>\n      <td>0.906787</td>\n      <td>0.663934</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Patch requisite packages to the model environment YAML for model serving\nimport os\nimport shutil\nimport uuid\nimport yaml\n\nNone\n\nimport xgboost\nfrom mlflow.tracking import MlflowClient\n\nxgbc_temp_dir = os.path.join(os.environ[\"SPARK_LOCAL_DIRS\"], str(uuid.uuid4())[:8])\nos.makedirs(xgbc_temp_dir)\nxgbc_client = MlflowClient()\nxgbc_model_env_path = xgbc_client.download_artifacts(mlflow_run.info.run_id, \"model/conda.yaml\", xgbc_temp_dir)\nxgbc_model_env_str = open(xgbc_model_env_path)\nxgbc_parsed_model_env_str = yaml.load(xgbc_model_env_str, Loader=yaml.FullLoader)\n\nxgbc_parsed_model_env_str[\"dependencies\"][-1][\"pip\"].append(f\"xgboost=={xgboost.__version__}\")\n\nwith open(xgbc_model_env_path, \"w\") as f:\n  f.write(yaml.dump(xgbc_parsed_model_env_str))\nxgbc_client.log_artifact(run_id=mlflow_run.info.run_id, local_path=xgbc_model_env_path, artifact_path=\"model\")\nshutil.rmtree(xgbc_temp_dir)"],"metadata":{"execution":{"iopub.execute_input":"2022-03-28T00:51:32.603144Z","iopub.status.busy":"2022-03-28T00:51:32.602312Z","iopub.status.idle":"2022-03-28T00:51:33.080591Z","shell.execute_reply":"2022-03-28T00:51:33.081156Z"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b38a6ee-00c1-4586-91ef-c4ce86936b60"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Feature importance\n\nSHAP is a game-theoretic approach to explain machine learning models, providing a summary plot\nof the relationship between features and model output. Features are ranked in descending order of\nimportance, and impact/color describe the correlation between the feature and the target variable.\n- Generating SHAP feature importance is a very memory intensive operation, so to ensure that AutoML can run trials without\n  running out of memory, we disable SHAP by default.<br />\n  You can set the flag defined below to `shap_enabled = True` and re-run this notebook to see the SHAP plots.\n- To reduce the computational overhead of each trial, a single example is sampled from the validation set to explain.<br />\n  For more thorough results, increase the sample size of explanations, or provide your own examples to explain.\n- SHAP cannot explain models using data with nulls; if your dataset has any, both the background data and\n  examples to explain will be imputed using the mode (most frequent values). This affects the computed\n  SHAP values, as the imputed samples may not match the actual data distribution.\n\nFor more information on how to read Shapley values, see the [SHAP documentation](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5fa05ddd-7fdc-40e7-aee4-518831a9309e"}}},{"cell_type":"code","source":["# Set this flag to True and re-run the notebook to see the SHAP plots\nshap_enabled = False"],"metadata":{"execution":{"iopub.execute_input":"2022-03-28T00:51:33.085933Z","iopub.status.busy":"2022-03-28T00:51:33.085251Z","iopub.status.idle":"2022-03-28T00:51:33.088636Z","shell.execute_reply":"2022-03-28T00:51:33.093287Z"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef28c72f-47d3-4093-b355-ef9456877498"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["if shap_enabled:\n    from shap import KernelExplainer, summary_plot\n    # Sample background data for SHAP Explainer. Increase the sample size to reduce variance.\n    train_sample = X_train.sample(n=min(100, len(X_train.index)))\n\n    # Sample a single example from the validation set to explain. Increase the sample size and rerun for more thorough results.\n    example = X_val.sample(n=1)\n\n    # Use Kernel SHAP to explain feature importance on the example from the validation set.\n    predict = lambda x: model.predict_proba(pd.DataFrame(x, columns=X_train.columns))\n    explainer = KernelExplainer(predict, train_sample, link=\"logit\")\n    shap_values = explainer.shap_values(example, l1_reg=False)\n    summary_plot(shap_values, example, class_names=model.classes_)"],"metadata":{"execution":{"iopub.execute_input":"2022-03-28T00:51:33.102439Z","iopub.status.busy":"2022-03-28T00:51:33.101750Z","iopub.status.idle":"2022-03-28T00:51:33.104171Z","shell.execute_reply":"2022-03-28T00:51:33.104747Z"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c476753-8dd7-46f6-bc63-71bbacc4034f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Inference\n[The MLflow Model Registry](https://docs.databricks.com/applications/mlflow/model-registry.html) is a collaborative hub where teams can share ML models, work together from experimentation to online testing and production, integrate with approval and governance workflows, and monitor ML deployments and their performance. The snippets below show how to add the model trained in this notebook to the model registry and to retrieve it later for inference.\n\n> **NOTE:** The `model_uri` for the model already trained in this notebook can be found in the cell below\n\n### Register to Model Registry\n```\nmodel_name = \"Example\"\n\nmodel_uri = f\"runs:/{ mlflow_run.info.run_id }/model\"\nregistered_model_version = mlflow.register_model(model_uri, model_name)\n```\n\n### Load from Model Registry\n```\nmodel_name = \"Example\"\nmodel_version = registered_model_version.version\n\nmodel = mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}/{model_version}\")\nmodel.predict(input_X)\n```\n\n### Load model without registering\n```\nmodel_uri = f\"runs:/{ mlflow_run.info.run_id }/model\"\n\nmodel = mlflow.pyfunc.load_model(model_uri)\nmodel.predict(input_X)\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6019a71-4562-4162-9a23-ebb220191f99"}}},{"cell_type":"code","source":["# model_uri for the generated model\nprint(f\"runs:/{ mlflow_run.info.run_id }/model\")"],"metadata":{"execution":{"iopub.execute_input":"2022-03-28T00:51:33.109723Z","iopub.status.busy":"2022-03-28T00:51:33.108994Z","iopub.status.idle":"2022-03-28T00:51:33.113194Z","shell.execute_reply":"2022-03-28T00:51:33.112341Z"},"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56d3e9bb-bd8a-4fb6-86e0-f251091b535f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"runs:/6295d5a5af704769ac898c7e84de1d89/model\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["runs:/6295d5a5af704769ac898c7e84de1d89/model\n"]}}],"execution_count":0}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.8.10","nbconvert_exporter":"python","file_extension":".py"},"name":"XGBoost-9bf14a4db008c6a70457a282394987af","application/vnd.databricks.v1+notebook":{"notebookName":"22-03-28-00:47-XGBoost-9bf14a4db008c6a70457a282394987af","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":65649428653551}},"nbformat":4,"nbformat_minor":0}
